# Enhanced PostgreSQL Database Structure for LLM Notator

## Overview

This document describes the enhanced PostgreSQL database structure designed to capture all annotation data generated by the LLM Notator application.

## Database Schema

### Primary Table: `archives`

The main table storing all archived annotation data with the following structure:

```sql
CREATE TABLE archives (
    id SERIAL PRIMARY KEY,
    
    -- Core content
    prompt TEXT NOT NULL,
    response TEXT NOT NULL,
    
    -- Annotation data
    error_categories JSONB DEFAULT '[]'::JSONB,
    category_notes JSONB DEFAULT '{}'::JSONB,
    general_notes TEXT DEFAULT '',
    
    -- Tracking data
    example_id VARCHAR(255),
    annotator_session_id UUID DEFAULT uuid_generate_v4(),
    
    -- Computed metadata (automatically calculated)
    prompt_length INTEGER GENERATED ALWAYS AS (LENGTH(prompt)) STORED,
    response_length INTEGER GENERATED ALWAYS AS (LENGTH(response)) STORED,
    categories_count INTEGER GENERATED ALWAYS AS (jsonb_array_length(error_categories)) STORED,
    
    -- Timestamps
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    -- Data validation constraints
    CONSTRAINT valid_prompt CHECK (LENGTH(prompt) > 0),
    CONSTRAINT valid_response CHECK (LENGTH(response) > 0),
    CONSTRAINT valid_categories CHECK (jsonb_typeof(error_categories) = 'array'),
    CONSTRAINT valid_category_notes CHECK (jsonb_typeof(category_notes) = 'object')
);
```

### Field Descriptions

| Field | Type | Description |
|-------|------|-------------|
| `id` | SERIAL | Primary key, auto-incrementing identifier |
| `prompt` | TEXT | The LLM input prompt text |
| `response` | TEXT | The LLM generated response text |
| `error_categories` | JSONB | Array of error category labels selected during annotation |
| `category_notes` | JSONB | Object mapping category names to specific notes for each category |
| `general_notes` | TEXT | General annotation notes not specific to any category |
| `example_id` | VARCHAR(255) | Original example ID from the annotation system |
| `annotator_session_id` | UUID | Unique session identifier to track annotation sessions |
| `prompt_length` | INTEGER | Automatically calculated character count of prompt |
| `response_length` | INTEGER | Automatically calculated character count of response |
| `categories_count` | INTEGER | Automatically calculated count of selected error categories |
| `created_at` | TIMESTAMP WITH TIME ZONE | When the record was created |
| `updated_at` | TIMESTAMP WITH TIME ZONE | When the record was last updated |

### Indexes

The following indexes are created for optimal query performance:

```sql
-- Primary timestamp indexes
CREATE INDEX idx_archives_created_at ON archives(created_at DESC);
CREATE INDEX idx_archives_updated_at ON archives(updated_at DESC);

-- Tracking indexes
CREATE INDEX idx_archives_example_id ON archives(example_id);
CREATE INDEX idx_archives_session_id ON archives(annotator_session_id);

-- Full-text search indexes
CREATE INDEX idx_archives_prompt_fts ON archives USING gin(to_tsvector('english', prompt));
CREATE INDEX idx_archives_response_fts ON archives USING gin(to_tsvector('english', response));
CREATE INDEX idx_archives_notes_fts ON archives USING gin(to_tsvector('english', general_notes));

-- JSONB indexes
CREATE INDEX idx_archives_error_categories ON archives USING gin(error_categories);
CREATE INDEX idx_archives_category_notes ON archives USING gin(category_notes);

-- Composite indexes for common queries
CREATE INDEX idx_archives_date_categories ON archives(created_at DESC, error_categories);
```

### Analytics View

A summary view is provided for analytics:

```sql
CREATE VIEW archive_summary AS
SELECT 
    DATE_TRUNC('day', created_at) as date,
    COUNT(*) as total_annotations,
    COUNT(DISTINCT example_id) as unique_examples,
    COUNT(DISTINCT annotator_session_id) as unique_sessions,
    AVG(prompt_length) as avg_prompt_length,
    AVG(response_length) as avg_response_length,
    AVG(categories_count) as avg_categories_per_annotation,
    
    -- Most common error categories for the day
    (
        SELECT jsonb_agg(category)
        FROM (
            SELECT jsonb_array_elements_text(error_categories) as category
            FROM archives a2 
            WHERE DATE_TRUNC('day', a2.created_at) = DATE_TRUNC('day', archives.created_at)
        ) categories
        GROUP BY category
        ORDER BY COUNT(*) DESC
        LIMIT 5
    ) as top_error_categories
FROM archives
GROUP BY DATE_TRUNC('day', created_at)
ORDER BY date DESC;
```

## API Endpoints

### Core CRUD Operations

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/health` | Health check |
| `GET` | `/archives` | Get archives with pagination |
| `GET` | `/archives/:id` | Get specific archive |
| `POST` | `/archives` | Create new archive |
| `PUT` | `/archives/:id` | Update existing archive |
| `DELETE` | `/archives/:id` | Delete archive |

### Enhanced Operations

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/archives/search/:query` | Full-text search with relevance ranking |
| `GET` | `/analytics` | Get analytics and statistics |
| `POST` | `/archives/bulk` | Bulk operations (delete, update) |
| `GET` | `/export` | Export data in JSON or CSV format |

### Search Capabilities

The enhanced search endpoint provides:

- **Full-text search** using PostgreSQL's built-in text search
- **Relevance ranking** for search results
- **Fallback to ILIKE** for compatibility
- **Multi-field search** across prompts, responses, notes, and categories
- **Search in JSONB fields** for error categories and category notes

## Data Examples

### Sample Archive Record

```json
{
  "id": 1,
  "prompt": "Explain quantum computing to a beginner",
  "response": "Quantum computing uses quantum mechanical phenomena...",
  "error_categories": ["accuracy", "clarity"],
  "category_notes": {
    "accuracy": "Some technical details could be more precise",
    "clarity": "Good overall explanation but could use more examples"
  },
  "general_notes": "Good introduction but needs more concrete examples",
  "example_id": "llm_1750911120266_ncd507b1u",
  "annotator_session_id": "550e8400-e29b-41d4-a716-446655440000",
  "prompt_length": 39,
  "response_length": 256,
  "categories_count": 2,
  "created_at": "2024-01-15T10:30:00Z",
  "updated_at": "2024-01-15T10:30:00Z"
}
```

### Analytics Response Example

```json
{
  "summary": {
    "total_archives": 150,
    "unique_examples": 120,
    "unique_sessions": 5,
    "avg_prompt_length": 45.6,
    "avg_response_length": 234.8,
    "avg_categories_per_annotation": 2.3,
    "earliest_annotation": "2024-01-01T00:00:00Z",
    "latest_annotation": "2024-01-15T00:00:00Z"
  },
  "topErrorCategories": [
    {"category": "accuracy", "frequency": 45},
    {"category": "clarity", "frequency": 38},
    {"category": "helpfulness", "frequency": 32}
  ],
  "dailyActivity": [
    {"date": "2024-01-15T00:00:00Z", "annotations_count": 12},
    {"date": "2024-01-14T00:00:00Z", "annotations_count": 8}
  ]
}
```

## Migration

The database structure includes automatic migration from the previous simple schema:

1. **Backup existing data** from the old `archives` table
2. **Create new enhanced table** with all new fields
3. **Migrate existing data** mapping `notes` â†’ `general_notes`
4. **Drop old table** and rename new table
5. **Create all indexes** for optimal performance

## Performance Considerations

### Query Optimization

- **Use indexes effectively**: The database includes comprehensive indexes for common query patterns
- **Leverage computed columns**: Use the generated length and count fields for filtering
- **Full-text search**: Use the FTS indexes for text-heavy searches
- **JSONB operations**: Efficiently query error categories and category notes

### Scaling Recommendations

- **Partitioning**: Consider date-based partitioning for very large datasets
- **Archival**: Move old data to separate archive tables if needed
- **Connection pooling**: Use connection pooling for high-concurrency scenarios
- **Monitoring**: Monitor query performance and index usage

## Security Considerations

- **Input validation**: All inputs are validated at the application level
- **SQL injection protection**: All queries use parameterized statements
- **Access control**: Implement appropriate user permissions (commented in migration)
- **Data backup**: Regular backup procedures should be established

## Maintenance

### Regular Tasks

1. **Analyze table statistics** for query optimization
2. **Monitor index usage** and add/remove as needed
3. **Vacuum and analyze** for optimal performance
4. **Backup data** regularly

### Monitoring Queries

```sql
-- Check table size and index usage
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables 
WHERE tablename = 'archives';

-- Monitor most common error categories
SELECT 
    category,
    COUNT(*) as frequency
FROM (
    SELECT jsonb_array_elements_text(error_categories) as category
    FROM archives
) categories
GROUP BY category
ORDER BY frequency DESC;

-- Check annotation activity by day
SELECT 
    DATE_TRUNC('day', created_at) as date,
    COUNT(*) as annotations
FROM archives
WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY DATE_TRUNC('day', created_at)
ORDER BY date;
```

This enhanced database structure provides comprehensive data capture, efficient querying capabilities, and strong analytics support for the LLM Notator application. 